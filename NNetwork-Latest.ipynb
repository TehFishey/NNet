{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDos:\n",
    "\n",
    "DONE: Add bias unit to backpropogation/updates (as w[0], or as a distinct variable?)\n",
    "- Allow for variable cost functions\n",
    "- Relabel delta, error variables to be more accurate to mathematical models\n",
    "- Add check for determining when training is complete?\n",
    "- Add optional internal regularization of inputs? (Or is this better kept external?)\n",
    "- Why can't it work with a single training example? Where is error coming from? I still don't get it :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### NETWORK CLASSES\n",
    "\n",
    "class nNetwork(object):\n",
    "    def __init__(self, numX, numY, hlayers, activation, cost=(lambda Y, o : Y - o)):\n",
    "        # Define activation function. \n",
    "        # Expects 'activation' to be a class with forward() and backward() static methods\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Expects a function that can take labels, outputs, and return error\n",
    "        self.cost = cost\n",
    "        \n",
    "        # Initialize network layers as a list of nlayer objects\n",
    "        # Expects 'hlayers' to be a list of integers; each being the size of each layer\n",
    "        self.layer = [ nLayer(i, activation) for i in hlayers ]\n",
    "        self.layer.insert(0, nLayer(numX, activation))\n",
    "        self.layer.append(nLayer(numY, activation))\n",
    "        \n",
    "        # Initialize WEIGHT MATRICIES and BIASES for each layer\n",
    "        # WEIGHT MATRICIES are the theta multipliers applied to the\n",
    "        # PREVIOUS layer's output to determine the CURRENT layer's activations\n",
    "        # Each node is represented by a row; each input is a column\n",
    "        for i in range(1, len(self.layer)):\n",
    "            prevnodes = self.layer[i-1].len\n",
    "            curnodes = self.layer[i].len\n",
    "            # Add an additional set of weights to serve as biases for each node in this layer\n",
    "            self.layer[i].w = np.random.randn(prevnodes + 1, curnodes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Input layer activation\n",
    "        a = self.layer[0].input_activation(X)\n",
    "        \n",
    "        # Hidden and output layer activation\n",
    "        for l in self.layer[1:]:\n",
    "            a = l.activation(a)\n",
    "        \n",
    "        # Clean bias unit from output layer\n",
    "        self.layer[-1].a = np.delete(self.layer[-1].a, 0, axis=1) \n",
    "        return a\n",
    "    \n",
    "    def backward(self, Y, rate):\n",
    "        prime = self.activation.backward\n",
    "        cost = self.cost\n",
    "        \n",
    "        # Calculate delta for all layers\n",
    "        for l in self.layer[::-1]:\n",
    "            if l == self.layer[-1]:\n",
    "                # For output layer:\n",
    "                # Calculate error (labels - predictions)\n",
    "                # Calculate delta\n",
    "                l.error = cost(Y, l.a)\n",
    "                l.delta = l.error*prime(l.a)\n",
    "                pre_d = l.delta\n",
    "                pre_w = l.w\n",
    "                \n",
    "            elif l == self.layer[0]:\n",
    "                # For input layer:\n",
    "                # Do nothing\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                # For hidden layers:\n",
    "                # Calculate error (apply succeeding layer's delta to adjoined weights)\n",
    "                # Calculate delta\n",
    "                l.error = pre_d @ pre_w.T[:, 1:]    # Don't backpropogate from the Bias node!\n",
    "                l.delta = l.error*prime(l.a[:, 1:]) # Don't backpropogate from the Bias node!\n",
    "                pre_d = l.delta\n",
    "                pre_w = l.w\n",
    "        \n",
    "        # Apply delta for all layers\n",
    "        for l in self.layer:\n",
    "            if l == self.layer[0]:\n",
    "                # For input layer:\n",
    "                # Do nothing\n",
    "                pre_a = l.a\n",
    "            else:\n",
    "                # For hidden & output layers:\n",
    "                # Set weights to weights + deltas * inputs * learning rate\n",
    "                l.w += (pre_a.T @ l.delta) * rate\n",
    "                pre_a = l.a          \n",
    "            \n",
    "            \n",
    "    def train (self, X, Y, epochs, rate=1):\n",
    "        print(f'Training for {epochs} epochs at α={rate}:')\n",
    "        for i in range(0,epochs):\n",
    "            a = self.forward(X)\n",
    "            self.backward(Y, rate)\n",
    "            print(f'Epoch {i+1} Loss: {np.mean(np.square(Y - a))}')\n",
    "    \n",
    "    def describe(self):\n",
    "        # Descriptive tool for displaying network's size, shape, etc.\n",
    "        out = ''\n",
    "        \n",
    "        print(f'Input Nodes (X): {self.layer[0].len}')\n",
    "        print(f'Output Nodes (Y): {self.layer[-1].len}')\n",
    "        print(f'Hidden Layers: {len(self.layer[1:-1])} Total')\n",
    "        print(f'   Sizes: {[l.len for l in self.layer[1:-1]]}')\n",
    "        print('\\nNetwork Shape:')\n",
    "        for index, item in enumerate(self.layer):\n",
    "            if index == 0:\n",
    "                out += str(item.len) + 'X '\n",
    "            elif index == len(self.layer)-1:\n",
    "                out += '| ' + str(item.len) + 'Y'\n",
    "            else:\n",
    "                out += '| ' + str(item.len) + ' '\n",
    "        print(out)\n",
    "        \n",
    "\n",
    "class nLayer(object):\n",
    "    def __init__(self, nodecount, activation):\n",
    "        self.len = nodecount         # Number of nodes/perceptrons. Used for reports. \n",
    "        self.w = None                # Weight matrix (declared in nNetwork object)\n",
    "        self.z = None                # (Dot product of inputs x weight matrix) + bias'\n",
    "        self.a = None                # 1D array of unit outputs\n",
    "        self.error = None            # Layer error\n",
    "        self.delta = None            # 1D array of deltas, for backprop\n",
    "        self.g = activation.forward  # Layer activation function\n",
    "        \n",
    "    def activation(self, i):\n",
    "        self.z = i @ self.w\n",
    "        self.a = self.g(self.z)\n",
    "        self.a = np.hstack((np.ones((np.size(self.a,0),1)), self.a))\n",
    "        return self.a\n",
    "    \n",
    "    def input_activation(self, i):\n",
    "        self.z = i\n",
    "        self.a = self.z\n",
    "        self.a = np.hstack((np.ones((np.size(self.a,0),1)), self.a))\n",
    "        return self.a\n",
    "    \n",
    "    def describe(self):\n",
    "        print(f'Layer Nodes: {self.len}')\n",
    "        print(f'\\nw (input weights): {len(self.w)} (inputs) x {len(self.w.T)} (nodes)\\n{self.w}')\n",
    "        print(f'\\nz (weighted sums): {len(self.z)}\\n{self.z}')\n",
    "        print(f'\\na (outputs): {len(self.a)}\\n{self.a}')\n",
    "        print(f'\\nerrors: {len(self.error)}\\n{self.error}')\n",
    "        print(f'\\ndeltas: {len(self.delta)}\\n{self.delta}')\n",
    "        print(f'\\nactivation function: {self.g}')\n",
    "\n",
    "        \n",
    "        \n",
    "### ACTIVATION FUNCTIONS (CLASSES)       \n",
    "\n",
    "class sigmoid(object):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    \n",
    "    # TECHINICALLY, sigmoid' is sigmoid(x) * (1 - sigmoid(x))\n",
    "    # However, we are applying this to the nNetwork.layer.a values, \n",
    "    # which have already had the sigmoid function applied to them.\n",
    "    # Therefore, we will use x * (1 - x) for backpropogation.\n",
    "    @staticmethod\n",
    "    def backward(x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "### COST FUNCTIONS\n",
    "\n",
    "def crossentropy(Y, o):\n",
    "    # Cost funtion for use in classification problems\n",
    "    if Y == 1:\n",
    "        return -(np.log(o))\n",
    "    else:\n",
    "        return -(np.log(1 - o))\n",
    "    \n",
    "    \n",
    "    \n",
    "### SUPPORT FUNCTIONS\n",
    "\n",
    "def scale(X, Y, Xmax=None, Ymax=None):\n",
    "    if not Xmax:\n",
    "        Xmax = np.amax(X, axis=0)\n",
    "    if not Ymax:\n",
    "        Ymax = np.amax(Y, axis=0)\n",
    "    \n",
    "    X_b = X/Xmax\n",
    "    Y_b = Y/Ymax\n",
    "    \n",
    "    return X_b, Y_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 epochs at α=1:\n",
      "Epoch 1 Loss: 0.20075869227062476\n",
      "Epoch 2 Loss: 0.1410189076591983\n",
      "Epoch 3 Loss: 0.08489711804201441\n",
      "Epoch 4 Loss: 0.047677578493861034\n",
      "Epoch 5 Loss: 0.028898445123512917\n",
      "Epoch 6 Loss: 0.01995162910869915\n",
      "Epoch 7 Loss: 0.015357925867988464\n",
      "Epoch 8 Loss: 0.01276446251977115\n",
      "Epoch 9 Loss: 0.011178417581281238\n",
      "Epoch 10 Loss: 0.01014603984427185\n",
      "Epoch 11 Loss: 0.009440710213855261\n",
      "Epoch 12 Loss: 0.00894010645884216\n",
      "Epoch 13 Loss: 0.008573798269424455\n",
      "Epoch 14 Loss: 0.008299013723254325\n",
      "Epoch 15 Loss: 0.008088602946803136\n",
      "Epoch 16 Loss: 0.007924681513685724\n",
      "Epoch 17 Loss: 0.0077950927769651\n",
      "Epoch 18 Loss: 0.007691348610309258\n",
      "Epoch 19 Loss: 0.007607383398331416\n",
      "Epoch 20 Loss: 0.007538774314962442\n",
      "Epoch 21 Loss: 0.007482238818556664\n",
      "Epoch 22 Loss: 0.007435302294772414\n",
      "Epoch 23 Loss: 0.007396073119859858\n",
      "Epoch 24 Loss: 0.007363087268079259\n",
      "Epoch 25 Loss: 0.007335198966827321\n",
      "Epoch 26 Loss: 0.00731150246566696\n",
      "Epoch 27 Loss: 0.007291275217502467\n",
      "Epoch 28 Loss: 0.007273936042699758\n",
      "Epoch 29 Loss: 0.007259013937844008\n",
      "Epoch 30 Loss: 0.007246124552896952\n",
      "Epoch 31 Loss: 0.007234952263718223\n",
      "Epoch 32 Loss: 0.0072252363757016255\n",
      "Epoch 33 Loss: 0.007216760410847613\n",
      "Epoch 34 Loss: 0.007209343719638279\n",
      "Epoch 35 Loss: 0.0072028348622495035\n",
      "Epoch 36 Loss: 0.007197106348168546\n",
      "Epoch 37 Loss: 0.0071920504272656935\n",
      "Epoch 38 Loss: 0.00718757570096101\n",
      "Epoch 39 Loss: 0.0071836043776243065\n",
      "Epoch 40 Loss: 0.007180070037466265\n",
      "Epoch 41 Loss: 0.007176915802911168\n",
      "Epoch 42 Loss: 0.007174092833597868\n",
      "Epoch 43 Loss: 0.0071715590827387985\n",
      "Epoch 44 Loss: 0.007169278265014357\n",
      "Epoch 45 Loss: 0.007167218996536569\n",
      "Epoch 46 Loss: 0.00716535407544318\n",
      "Epoch 47 Loss: 0.0071636598779443155\n",
      "Epoch 48 Loss: 0.00716211584955545\n",
      "Epoch 49 Loss: 0.007160704075125387\n",
      "Epoch 50 Loss: 0.007159408914341021\n",
      "Epoch 51 Loss: 0.007158216691840389\n",
      "Epoch 52 Loss: 0.007157115433027598\n",
      "Epoch 53 Loss: 0.00715609463826227\n",
      "Epoch 54 Loss: 0.007155145089372022\n",
      "Epoch 55 Loss: 0.007154258683472504\n",
      "Epoch 56 Loss: 0.007153428289923484\n",
      "Epoch 57 Loss: 0.00715264762693996\n",
      "Epoch 58 Loss: 0.007151911154944453\n",
      "Epoch 59 Loss: 0.007151213984213642\n",
      "Epoch 60 Loss: 0.0071505517947591715\n",
      "Epoch 61 Loss: 0.0071499207667026195\n",
      "Epoch 62 Loss: 0.007149317519671727\n",
      "Epoch 63 Loss: 0.007148739059967321\n",
      "Epoch 64 Loss: 0.007148182734436943\n",
      "Epoch 65 Loss: 0.007147646190147679\n",
      "Epoch 66 Loss: 0.0071471273390823304\n",
      "Epoch 67 Loss: 0.007146624327194278\n",
      "Epoch 68 Loss: 0.007146135507250495\n",
      "Epoch 69 Loss: 0.007145659414971918\n",
      "Epoch 70 Loss: 0.007145194748048138\n",
      "Epoch 71 Loss: 0.007144740347661317\n",
      "Epoch 72 Loss: 0.007144295182203373\n",
      "Epoch 73 Loss: 0.00714385833291286\n",
      "Epoch 74 Loss: 0.007143428981194096\n",
      "Epoch 75 Loss: 0.007143006397412097\n",
      "Epoch 76 Loss: 0.007142589930983826\n",
      "Epoch 77 Loss: 0.007142179001609084\n",
      "Epoch 78 Loss: 0.007141773091504497\n",
      "Epoch 79 Loss: 0.007141371738521174\n",
      "Epoch 80 Loss: 0.007140974530041548\n",
      "Epoch 81 Loss: 0.007140581097563867\n",
      "Epoch 82 Loss: 0.007140191111894074\n",
      "Epoch 83 Loss: 0.007139804278874645\n",
      "Epoch 84 Loss: 0.007139420335588351\n",
      "Epoch 85 Loss: 0.007139039046982526\n",
      "Epoch 86 Loss: 0.007138660202865844\n",
      "Epoch 87 Loss: 0.007138283615235253\n",
      "Epoch 88 Loss: 0.007137909115895841\n",
      "Epoch 89 Loss: 0.007137536554340608\n",
      "Epoch 90 Loss: 0.007137165795861103\n",
      "Epoch 91 Loss: 0.007136796719863124\n",
      "Epoch 92 Loss: 0.00713642921836477\n",
      "Epoch 93 Loss: 0.007136063194656598\n",
      "Epoch 94 Loss: 0.007135698562106099\n",
      "Epoch 95 Loss: 0.007135335243090576\n",
      "Epoch 96 Loss: 0.007134973168044385\n",
      "Epoch 97 Loss: 0.0071346122746081065\n",
      "Epoch 98 Loss: 0.007134252506868476\n",
      "Epoch 99 Loss: 0.00713389381467929\n",
      "Epoch 100 Loss: 0.0071335361530545345\n",
      "Epoch 101 Loss: 0.007133179481625896\n",
      "Epoch 102 Loss: 0.007132823764157752\n",
      "Epoch 103 Loss: 0.007132468968113491\n",
      "Epoch 104 Loss: 0.007132115064267632\n",
      "Epoch 105 Loss: 0.007131762026358858\n",
      "Epoch 106 Loss: 0.0071314098307796335\n",
      "Epoch 107 Loss: 0.007131058456298418\n",
      "Epoch 108 Loss: 0.007130707883811151\n",
      "Epoch 109 Loss: 0.0071303580961187725\n",
      "Epoch 110 Loss: 0.007130009077728105\n",
      "Epoch 111 Loss: 0.007129660814673595\n",
      "Epoch 112 Loss: 0.007129313294357699\n",
      "Epoch 113 Loss: 0.007128966505408006\n",
      "Epoch 114 Loss: 0.007128620437549242\n",
      "Epoch 115 Loss: 0.00712827508148871\n",
      "Epoch 116 Loss: 0.007127930428813631\n",
      "Epoch 117 Loss: 0.007127586471899216\n",
      "Epoch 118 Loss: 0.007127243203826351\n",
      "Epoch 119 Loss: 0.007126900618307811\n",
      "Epoch 120 Loss: 0.007126558709622214\n",
      "Epoch 121 Loss: 0.007126217472554825\n",
      "Epoch 122 Loss: 0.007125876902344534\n",
      "Epoch 123 Loss: 0.007125536994636344\n",
      "Epoch 124 Loss: 0.0071251977454388155\n",
      "Epoch 125 Loss: 0.007124859151085946\n",
      "Epoch 126 Loss: 0.007124521208203002\n",
      "Epoch 127 Loss: 0.007124183913675919\n",
      "Epoch 128 Loss: 0.007123847264623873\n",
      "Epoch 129 Loss: 0.007123511258374726\n",
      "Epoch 130 Loss: 0.007123175892443011\n",
      "Epoch 131 Loss: 0.007122841164510231\n",
      "Epoch 132 Loss: 0.00712250707240718\n",
      "Epoch 133 Loss: 0.007122173614098141\n",
      "Epoch 134 Loss: 0.007121840787666693\n",
      "Epoch 135 Loss: 0.007121508591303042\n",
      "Epoch 136 Loss: 0.007121177023292637\n",
      "Epoch 137 Loss: 0.0071208460820059985\n",
      "Epoch 138 Loss: 0.007120515765889609\n",
      "Epoch 139 Loss: 0.007120186073457733\n",
      "Epoch 140 Loss: 0.007119857003285125\n",
      "Epoch 141 Loss: 0.0071195285540004765\n",
      "Epoch 142 Loss: 0.007119200724280576\n",
      "Epoch 143 Loss: 0.007118873512845057\n",
      "Epoch 144 Loss: 0.007118546918451713\n",
      "Epoch 145 Loss: 0.007118220939892311\n",
      "Epoch 146 Loss: 0.007117895575988828\n",
      "Epoch 147 Loss: 0.007117570825590107\n",
      "Epoch 148 Loss: 0.007117246687568846\n",
      "Epoch 149 Loss: 0.00711692316081893\n",
      "Epoch 150 Loss: 0.007116600244253023\n",
      "Epoch 151 Loss: 0.007116277936800435\n",
      "Epoch 152 Loss: 0.0071159562374052115\n",
      "Epoch 153 Loss: 0.007115635145024425\n",
      "Epoch 154 Loss: 0.007115314658626638\n",
      "Epoch 155 Loss: 0.007114994777190579\n",
      "Epoch 156 Loss: 0.007114675499703895\n",
      "Epoch 157 Loss: 0.007114356825162087\n",
      "Epoch 158 Loss: 0.007114038752567551\n",
      "Epoch 159 Loss: 0.007113721280928716\n",
      "Epoch 160 Loss: 0.00711340440925929\n",
      "Epoch 161 Loss: 0.007113088136577561\n",
      "Epoch 162 Loss: 0.007112772461905817\n",
      "Epoch 163 Loss: 0.007112457384269814\n",
      "Epoch 164 Loss: 0.007112142902698281\n",
      "Epoch 165 Loss: 0.007111829016222511\n",
      "Epoch 166 Loss: 0.007111515723875994\n",
      "Epoch 167 Loss: 0.007111203024694084\n",
      "Epoch 168 Loss: 0.007110890917713708\n",
      "Epoch 169 Loss: 0.007110579401973093\n",
      "Epoch 170 Loss: 0.0071102684765115875\n",
      "Epoch 171 Loss: 0.0071099581403694075\n",
      "Epoch 172 Loss: 0.007109648392587503\n",
      "Epoch 173 Loss: 0.007109339232207383\n",
      "Epoch 174 Loss: 0.007109030658271005\n",
      "Epoch 175 Loss: 0.007108722669820632\n",
      "Epoch 176 Loss: 0.007108415265898754\n",
      "Epoch 177 Loss: 0.007108108445548001\n",
      "Epoch 178 Loss: 0.0071078022078110556\n",
      "Epoch 179 Loss: 0.007107496551730594\n",
      "Epoch 180 Loss: 0.007107191476349244\n",
      "Epoch 181 Loss: 0.0071068869807095324\n",
      "Epoch 182 Loss: 0.007106583063853839\n",
      "Epoch 183 Loss: 0.007106279724824378\n",
      "Epoch 184 Loss: 0.007105976962663168\n",
      "Epoch 185 Loss: 0.007105674776412007\n",
      "Epoch 186 Loss: 0.007105373165112469\n",
      "Epoch 187 Loss: 0.007105072127805885\n",
      "Epoch 188 Loss: 0.00710477166353333\n",
      "Epoch 189 Loss: 0.007104471771335637\n",
      "Epoch 190 Loss: 0.007104172450253368\n",
      "Epoch 191 Loss: 0.007103873699326839\n",
      "Epoch 192 Loss: 0.007103575517596119\n",
      "Epoch 193 Loss: 0.007103277904101017\n",
      "Epoch 194 Loss: 0.007102980857881104\n",
      "Epoch 195 Loss: 0.007102684377975723\n",
      "Epoch 196 Loss: 0.007102388463423984\n",
      "Epoch 197 Loss: 0.007102093113264787\n",
      "Epoch 198 Loss: 0.007101798326536821\n",
      "Epoch 199 Loss: 0.007101504102278583\n",
      "Epoch 200 Loss: 0.007101210439528394\n",
      "Epoch 201 Loss: 0.007100917337324395\n",
      "Epoch 202 Loss: 0.007100624794704581\n",
      "Epoch 203 Loss: 0.007100332810706798\n",
      "Epoch 204 Loss: 0.007100041384368758\n",
      "Epoch 205 Loss: 0.00709975051472807\n",
      "Epoch 206 Loss: 0.007099460200822228\n",
      "Epoch 207 Loss: 0.00709917044168864\n",
      "Epoch 208 Loss: 0.007098881236364646\n",
      "Epoch 209 Loss: 0.0070985925838875225\n",
      "Epoch 210 Loss: 0.007098304483294501\n",
      "Epoch 211 Loss: 0.007098016933622782\n",
      "Epoch 212 Loss: 0.007097729933909543\n",
      "Epoch 213 Loss: 0.007097443483191966\n",
      "Epoch 214 Loss: 0.007097157580507236\n",
      "Epoch 215 Loss: 0.0070968722248925635\n",
      "Epoch 216 Loss: 0.007096587415385213\n",
      "Epoch 217 Loss: 0.007096303151022472\n",
      "Epoch 218 Loss: 0.0070960194308417185\n",
      "Epoch 219 Loss: 0.007095736253880396\n",
      "Epoch 220 Loss: 0.0070954536191760505\n",
      "Epoch 221 Loss: 0.0070951715257663184\n",
      "Epoch 222 Loss: 0.007094889972688965\n",
      "Epoch 223 Loss: 0.0070946089589818935\n",
      "Epoch 224 Loss: 0.007094328483683133\n",
      "Epoch 225 Loss: 0.007094048545830881\n",
      "Epoch 226 Loss: 0.007093769144463505\n",
      "Epoch 227 Loss: 0.007093490278619556\n",
      "Epoch 228 Loss: 0.007093211947337771\n",
      "Epoch 229 Loss: 0.00709293414965709\n",
      "Epoch 230 Loss: 0.007092656884616685\n",
      "Epoch 231 Loss: 0.007092380151255956\n",
      "Epoch 232 Loss: 0.007092103948614534\n",
      "Epoch 233 Loss: 0.007091828275732301\n",
      "Epoch 234 Loss: 0.007091553131649424\n",
      "Epoch 235 Loss: 0.00709127851540633\n",
      "Epoch 236 Loss: 0.00709100442604373\n",
      "Epoch 237 Loss: 0.007090730862602644\n",
      "Epoch 238 Loss: 0.007090457824124393\n",
      "Epoch 239 Loss: 0.007090185309650628\n",
      "Epoch 240 Loss: 0.007089913318223322\n",
      "Epoch 241 Loss: 0.0070896418488847895\n",
      "Epoch 242 Loss: 0.007089370900677704\n",
      "Epoch 243 Loss: 0.007089100472645095\n",
      "Epoch 244 Loss: 0.007088830563830364\n",
      "Epoch 245 Loss: 0.007088561173277298\n",
      "Epoch 246 Loss: 0.007088292300030081\n",
      "Epoch 247 Loss: 0.007088023943133296\n",
      "Epoch 248 Loss: 0.007087756101631934\n",
      "Epoch 249 Loss: 0.007087488774571422\n",
      "Epoch 250 Loss: 0.007087221960997602\n",
      "Epoch 251 Loss: 0.007086955659956776\n",
      "Epoch 252 Loss: 0.007086689870495683\n",
      "Epoch 253 Loss: 0.007086424591661529\n",
      "Epoch 254 Loss: 0.0070861598225019995\n",
      "Epoch 255 Loss: 0.007085895562065243\n",
      "Epoch 256 Loss: 0.0070856318093999076\n",
      "Epoch 257 Loss: 0.007085368563555137\n",
      "Epoch 258 Loss: 0.007085105823580585\n",
      "Epoch 259 Loss: 0.007084843588526416\n",
      "Epoch 260 Loss: 0.007084581857443324\n",
      "Epoch 261 Loss: 0.007084320629382538\n",
      "Epoch 262 Loss: 0.007084059903395818\n",
      "Epoch 263 Loss: 0.007083799678535499\n",
      "Epoch 264 Loss: 0.007083539953854448\n",
      "Epoch 265 Loss: 0.007083280728406125\n",
      "Epoch 266 Loss: 0.00708302200124455\n",
      "Epoch 267 Loss: 0.0070827637714243365\n",
      "Epoch 268 Loss: 0.0070825060380006865\n",
      "Epoch 269 Loss: 0.007082248800029413\n",
      "Epoch 270 Loss: 0.007081992056566924\n",
      "Epoch 271 Loss: 0.007081735806670262\n",
      "Epoch 272 Loss: 0.00708148004939709\n",
      "Epoch 273 Loss: 0.007081224783805698\n",
      "Epoch 274 Loss: 0.007080970008955026\n",
      "Epoch 275 Loss: 0.0070807157239046615\n",
      "Epoch 276 Loss: 0.007080461927714855\n",
      "Epoch 277 Loss: 0.007080208619446508\n",
      "Epoch 278 Loss: 0.007079955798161216\n",
      "Epoch 279 Loss: 0.007079703462921236\n",
      "Epoch 280 Loss: 0.0070794516127895245\n",
      "Epoch 281 Loss: 0.007079200246829738\n",
      "Epoch 282 Loss: 0.007078949364106221\n",
      "Epoch 283 Loss: 0.007078698963684044\n",
      "Epoch 284 Loss: 0.007078449044628985\n",
      "Epoch 285 Loss: 0.007078199606007547\n",
      "Epoch 286 Loss: 0.007077950646886977\n",
      "Epoch 287 Loss: 0.007077702166335253\n",
      "Epoch 288 Loss: 0.007077454163421101\n",
      "Epoch 289 Loss: 0.0070772066372139925\n",
      "Epoch 290 Loss: 0.007076959586784182\n",
      "Epoch 291 Loss: 0.00707671301120267\n",
      "Epoch 292 Loss: 0.00707646690954124\n",
      "Epoch 293 Loss: 0.007076221280872458\n",
      "Epoch 294 Loss: 0.007075976124269668\n",
      "Epoch 295 Loss: 0.007075731438807028\n",
      "Epoch 296 Loss: 0.0070754872235594774\n",
      "Epoch 297 Loss: 0.007075243477602783\n",
      "Epoch 298 Loss: 0.007075000200013505\n",
      "Epoch 299 Loss: 0.00707475738986904\n",
      "Epoch 300 Loss: 0.007074515046247604\n",
      "Epoch 301 Loss: 0.007074273168228253\n",
      "Epoch 302 Loss: 0.007074031754890887\n",
      "Epoch 303 Loss: 0.007073790805316231\n",
      "Epoch 304 Loss: 0.007073550318585893\n",
      "Epoch 305 Loss: 0.007073310293782313\n",
      "Epoch 306 Loss: 0.007073070729988813\n",
      "Epoch 307 Loss: 0.0070728316262895765\n",
      "Epoch 308 Loss: 0.007072592981769668\n",
      "Epoch 309 Loss: 0.007072354795515038\n",
      "Epoch 310 Loss: 0.007072117066612522\n",
      "Epoch 311 Loss: 0.007071879794149852\n",
      "Epoch 312 Loss: 0.007071642977215658\n",
      "Epoch 313 Loss: 0.007071406614899479\n",
      "Epoch 314 Loss: 0.0070711707062917645\n",
      "Epoch 315 Loss: 0.0070709352504838814\n",
      "Epoch 316 Loss: 0.007070700246568128\n",
      "Epoch 317 Loss: 0.007070465693637713\n",
      "Epoch 318 Loss: 0.007070231590786803\n",
      "Epoch 319 Loss: 0.007069997937110489\n",
      "Epoch 320 Loss: 0.0070697647317048045\n",
      "Epoch 321 Loss: 0.007069531973666748\n",
      "Epoch 322 Loss: 0.007069299662094266\n",
      "Epoch 323 Loss: 0.007069067796086268\n",
      "Epoch 324 Loss: 0.007068836374742628\n",
      "Epoch 325 Loss: 0.007068605397164187\n",
      "Epoch 326 Loss: 0.00706837486245278\n",
      "Epoch 327 Loss: 0.007068144769711203\n",
      "Epoch 328 Loss: 0.0070679151180432585\n",
      "Epoch 329 Loss: 0.007067685906553722\n",
      "Epoch 330 Loss: 0.007067457134348383\n",
      "Epoch 331 Loss: 0.007067228800534021\n",
      "Epoch 332 Loss: 0.007067000904218428\n",
      "Epoch 333 Loss: 0.007066773444510409\n",
      "Epoch 334 Loss: 0.007066546420519782\n",
      "Epoch 335 Loss: 0.007066319831357384\n",
      "Epoch 336 Loss: 0.007066093676135087\n",
      "Epoch 337 Loss: 0.007065867953965779\n",
      "Epoch 338 Loss: 0.007065642663963404\n",
      "Epoch 339 Loss: 0.007065417805242916\n",
      "Epoch 340 Loss: 0.007065193376920342\n",
      "Epoch 341 Loss: 0.007064969378112741\n",
      "Epoch 342 Loss: 0.007064745807938227\n",
      "Epoch 343 Loss: 0.007064522665515983\n",
      "Epoch 344 Loss: 0.007064299949966234\n",
      "Epoch 345 Loss: 0.007064077660410288\n",
      "Epoch 346 Loss: 0.007063855795970509\n",
      "Epoch 347 Loss: 0.007063634355770349\n",
      "Epoch 348 Loss: 0.0070634133389343295\n",
      "Epoch 349 Loss: 0.007063192744588057\n",
      "Epoch 350 Loss: 0.007062972571858223\n",
      "Epoch 351 Loss: 0.007062752819872613\n",
      "Epoch 352 Loss: 0.007062533487760104\n",
      "Epoch 353 Loss: 0.007062314574650673\n",
      "Epoch 354 Loss: 0.007062096079675407\n",
      "Epoch 355 Loss: 0.007061878001966478\n",
      "Epoch 356 Loss: 0.007061660340657187\n",
      "Epoch 357 Loss: 0.0070614430948819475\n",
      "Epoch 358 Loss: 0.007061226263776282\n",
      "Epoch 359 Loss: 0.00706100984647684\n",
      "Epoch 360 Loss: 0.007060793842121395\n",
      "Epoch 361 Loss: 0.007060578249848847\n",
      "Epoch 362 Loss: 0.007060363068799238\n",
      "Epoch 363 Loss: 0.007060148298113724\n",
      "Epoch 364 Loss: 0.007059933936934622\n",
      "Epoch 365 Loss: 0.007059719984405383\n",
      "Epoch 366 Loss: 0.0070595064396705995\n",
      "Epoch 367 Loss: 0.007059293301876022\n",
      "Epoch 368 Loss: 0.007059080570168545\n",
      "Epoch 369 Loss: 0.007058868243696223\n",
      "Epoch 370 Loss: 0.0070586563216082756\n",
      "Epoch 371 Loss: 0.007058444803055077\n",
      "Epoch 372 Loss: 0.007058233687188162\n",
      "Epoch 373 Loss: 0.007058022973160245\n",
      "Epoch 374 Loss: 0.007057812660125211\n",
      "Epoch 375 Loss: 0.007057602747238107\n",
      "Epoch 376 Loss: 0.007057393233655177\n",
      "Epoch 377 Loss: 0.007057184118533828\n",
      "Epoch 378 Loss: 0.007056975401032665\n",
      "Epoch 379 Loss: 0.007056767080311472\n",
      "Epoch 380 Loss: 0.007056559155531216\n",
      "Epoch 381 Loss: 0.007056351625854069\n",
      "Epoch 382 Loss: 0.007056144490443395\n",
      "Epoch 383 Loss: 0.0070559377484637475\n",
      "Epoch 384 Loss: 0.007055731399080889\n",
      "Epoch 385 Loss: 0.007055525441461782\n",
      "Epoch 386 Loss: 0.007055319874774592\n",
      "Epoch 387 Loss: 0.007055114698188702\n",
      "Epoch 388 Loss: 0.007054909910874694\n",
      "Epoch 389 Loss: 0.007054705512004369\n",
      "Epoch 390 Loss: 0.0070545015007507485\n",
      "Epoch 391 Loss: 0.007054297876288068\n",
      "Epoch 392 Loss: 0.007054094637791773\n",
      "Epoch 393 Loss: 0.0070538917844385545\n",
      "Epoch 394 Loss: 0.007053689315406316\n",
      "Epoch 395 Loss: 0.007053487229874186\n",
      "Epoch 396 Loss: 0.007053285527022522\n",
      "Epoch 397 Loss: 0.007053084206032933\n",
      "Epoch 398 Loss: 0.007052883266088238\n",
      "Epoch 399 Loss: 0.0070526827063725085\n",
      "Epoch 400 Loss: 0.007052482526071048\n",
      "Epoch 401 Loss: 0.0070522827243704005\n",
      "Epoch 402 Loss: 0.007052083300458363\n",
      "Epoch 403 Loss: 0.0070518842535239575\n",
      "Epoch 404 Loss: 0.0070516855827574745\n",
      "Epoch 405 Loss: 0.007051487287350441\n",
      "Epoch 406 Loss: 0.007051289366495642\n",
      "Epoch 407 Loss: 0.007051091819387112\n",
      "Epoch 408 Loss: 0.007050894645220138\n",
      "Epoch 409 Loss: 0.007050697843191268\n",
      "Epoch 410 Loss: 0.007050501412498303\n",
      "Epoch 411 Loss: 0.007050305352340311\n",
      "Epoch 412 Loss: 0.007050109661917615\n",
      "Epoch 413 Loss: 0.007049914340431814\n",
      "Epoch 414 Loss: 0.007049719387085757\n",
      "Epoch 415 Loss: 0.007049524801083569\n",
      "Epoch 416 Loss: 0.0070493305816306355\n",
      "Epoch 417 Loss: 0.007049136727933632\n",
      "Epoch 418 Loss: 0.00704894323920048\n",
      "Epoch 419 Loss: 0.00704875011464039\n",
      "Epoch 420 Loss: 0.007048557353463845\n",
      "Epoch 421 Loss: 0.007048364954882605\n",
      "Epoch 422 Loss: 0.007048172918109705\n",
      "Epoch 423 Loss: 0.007047981242359457\n",
      "Epoch 424 Loss: 0.007047789926847461\n",
      "Epoch 425 Loss: 0.007047598970790589\n",
      "Epoch 426 Loss: 0.00704740837340701\n",
      "Epoch 427 Loss: 0.007047218133916165\n",
      "Epoch 428 Loss: 0.00704702825153878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429 Loss: 0.007046838725496881\n",
      "Epoch 430 Loss: 0.007046649555013772\n",
      "Epoch 431 Loss: 0.0070464607393140475\n",
      "Epoch 432 Loss: 0.007046272277623594\n",
      "Epoch 433 Loss: 0.007046084169169589\n",
      "Epoch 434 Loss: 0.007045896413180502\n",
      "Epoch 435 Loss: 0.007045709008886106\n",
      "Epoch 436 Loss: 0.007045521955517447\n",
      "Epoch 437 Loss: 0.007045335252306887\n",
      "Epoch 438 Loss: 0.007045148898488077\n",
      "Epoch 439 Loss: 0.00704496289329597\n",
      "Epoch 440 Loss: 0.007044777235966807\n",
      "Epoch 441 Loss: 0.007044591925738142\n",
      "Epoch 442 Loss: 0.0070444069618488155\n",
      "Epoch 443 Loss: 0.007044222343538989\n",
      "Epoch 444 Loss: 0.0070440380700501045\n",
      "Epoch 445 Loss: 0.007043854140624919\n",
      "Epoch 446 Loss: 0.007043670554507494\n",
      "Epoch 447 Loss: 0.0070434873109431854\n",
      "Epoch 448 Loss: 0.007043304409178668\n",
      "Epoch 449 Loss: 0.007043121848461911\n",
      "Epoch 450 Loss: 0.0070429396280421944\n",
      "Epoch 451 Loss: 0.007042757747170112\n",
      "Epoch 452 Loss: 0.007042576205097547\n",
      "Epoch 453 Loss: 0.0070423950010777125\n",
      "Epoch 454 Loss: 0.007042214134365117\n",
      "Epoch 455 Loss: 0.007042033604215581\n",
      "Epoch 456 Loss: 0.007041853409886237\n",
      "Epoch 457 Loss: 0.007041673550635532\n",
      "Epoch 458 Loss: 0.0070414940257232045\n",
      "Epoch 459 Loss: 0.007041314834410332\n",
      "Epoch 460 Loss: 0.007041135975959283\n",
      "Epoch 461 Loss: 0.007040957449633749\n",
      "Epoch 462 Loss: 0.00704077925469873\n",
      "Epoch 463 Loss: 0.007040601390420538\n",
      "Epoch 464 Loss: 0.0070404238560668\n",
      "Epoch 465 Loss: 0.007040246650906455\n",
      "Epoch 466 Loss: 0.007040069774209759\n",
      "Epoch 467 Loss: 0.007039893225248273\n",
      "Epoch 468 Loss: 0.007039717003294887\n",
      "Epoch 469 Loss: 0.00703954110762379\n",
      "Epoch 470 Loss: 0.007039365537510498\n",
      "Epoch 471 Loss: 0.007039190292231832\n",
      "Epoch 472 Loss: 0.007039015371065939\n",
      "Epoch 473 Loss: 0.007038840773292272\n",
      "Epoch 474 Loss: 0.007038666498191596\n",
      "Epoch 475 Loss: 0.007038492545046005\n",
      "Epoch 476 Loss: 0.007038318913138897\n",
      "Epoch 477 Loss: 0.007038145601754988\n",
      "Epoch 478 Loss: 0.007037972610180308\n",
      "Epoch 479 Loss: 0.007037799937702209\n",
      "Epoch 480 Loss: 0.007037627583609348\n",
      "Epoch 481 Loss: 0.007037455547191708\n",
      "Epoch 482 Loss: 0.007037283827740582\n",
      "Epoch 483 Loss: 0.007037112424548574\n",
      "Epoch 484 Loss: 0.0070369413369096075\n",
      "Epoch 485 Loss: 0.007036770564118928\n",
      "Epoch 486 Loss: 0.007036600105473077\n",
      "Epoch 487 Loss: 0.007036429960269932\n",
      "Epoch 488 Loss: 0.007036260127808662\n",
      "Epoch 489 Loss: 0.007036090607389777\n",
      "Epoch 490 Loss: 0.007035921398315077\n",
      "Epoch 491 Loss: 0.007035752499887694\n",
      "Epoch 492 Loss: 0.007035583911412053\n",
      "Epoch 493 Loss: 0.007035415632193909\n",
      "Epoch 494 Loss: 0.007035247661540328\n",
      "Epoch 495 Loss: 0.007035079998759686\n",
      "Epoch 496 Loss: 0.007034912643161665\n",
      "Epoch 497 Loss: 0.007034745594057264\n",
      "Epoch 498 Loss: 0.007034578850758801\n",
      "Epoch 499 Loss: 0.007034412412579891\n",
      "Epoch 500 Loss: 0.0070342462788354695\n",
      "Epoch 501 Loss: 0.007034080448841778\n",
      "Epoch 502 Loss: 0.0070339149219163675\n",
      "Epoch 503 Loss: 0.007033749697378101\n",
      "Epoch 504 Loss: 0.007033584774547152\n",
      "Epoch 505 Loss: 0.007033420152744994\n",
      "Epoch 506 Loss: 0.007033255831294414\n",
      "Epoch 507 Loss: 0.00703309180951951\n",
      "Epoch 508 Loss: 0.0070329280867456765\n",
      "Epoch 509 Loss: 0.0070327646622996235\n",
      "Epoch 510 Loss: 0.007032601535509369\n",
      "Epoch 511 Loss: 0.0070324387057042175\n",
      "Epoch 512 Loss: 0.007032276172214798\n",
      "Epoch 513 Loss: 0.007032113934373039\n",
      "Epoch 514 Loss: 0.007031951991512163\n",
      "Epoch 515 Loss: 0.0070317903429667015\n",
      "Epoch 516 Loss: 0.007031628988072493\n",
      "Epoch 517 Loss: 0.007031467926166662\n",
      "Epoch 518 Loss: 0.007031307156587645\n",
      "Epoch 519 Loss: 0.007031146678675185\n",
      "Epoch 520 Loss: 0.007030986491770297\n",
      "Epoch 521 Loss: 0.007030826595215325\n",
      "Epoch 522 Loss: 0.0070306669883538975\n",
      "Epoch 523 Loss: 0.007030507670530925\n",
      "Epoch 524 Loss: 0.007030348641092643\n",
      "Epoch 525 Loss: 0.007030189899386554\n",
      "Epoch 526 Loss: 0.0070300314447614785\n",
      "Epoch 527 Loss: 0.007029873276567511\n",
      "Epoch 528 Loss: 0.007029715394156049\n",
      "Epoch 529 Loss: 0.007029557796879777\n",
      "Epoch 530 Loss: 0.007029400484092673\n",
      "Epoch 531 Loss: 0.007029243455150008\n",
      "Epoch 532 Loss: 0.00702908670940833\n",
      "Epoch 533 Loss: 0.007028930246225485\n",
      "Epoch 534 Loss: 0.007028774064960609\n",
      "Epoch 535 Loss: 0.007028618164974111\n",
      "Epoch 536 Loss: 0.0070284625456276915\n",
      "Epoch 537 Loss: 0.0070283072062843465\n",
      "Epoch 538 Loss: 0.007028152146308335\n",
      "Epoch 539 Loss: 0.007027997365065214\n",
      "Epoch 540 Loss: 0.007027842861921807\n",
      "Epoch 541 Loss: 0.007027688636246234\n",
      "Epoch 542 Loss: 0.0070275346874078826\n",
      "Epoch 543 Loss: 0.007027381014777418\n",
      "Epoch 544 Loss: 0.007027227617726795\n",
      "Epoch 545 Loss: 0.007027074495629223\n",
      "Epoch 546 Loss: 0.007026921647859208\n",
      "Epoch 547 Loss: 0.0070267690737925124\n",
      "Epoch 548 Loss: 0.007026616772806185\n",
      "Epoch 549 Loss: 0.007026464744278533\n",
      "Epoch 550 Loss: 0.007026312987589142\n",
      "Epoch 551 Loss: 0.007026161502118867\n",
      "Epoch 552 Loss: 0.007026010287249825\n",
      "Epoch 553 Loss: 0.0070258593423654075\n",
      "Epoch 554 Loss: 0.007025708666850261\n",
      "Epoch 555 Loss: 0.007025558260090304\n",
      "Epoch 556 Loss: 0.007025408121472722\n",
      "Epoch 557 Loss: 0.007025258250385949\n",
      "Epoch 558 Loss: 0.007025108646219692\n",
      "Epoch 559 Loss: 0.007024959308364917\n",
      "Epoch 560 Loss: 0.007024810236213837\n",
      "Epoch 561 Loss: 0.007024661429159933\n",
      "Epoch 562 Loss: 0.007024512886597934\n",
      "Epoch 563 Loss: 0.007024364607923831\n",
      "Epoch 564 Loss: 0.007024216592534869\n",
      "Epoch 565 Loss: 0.00702406883982953\n",
      "Epoch 566 Loss: 0.007023921349207558\n",
      "Epoch 567 Loss: 0.007023774120069946\n",
      "Epoch 568 Loss: 0.007023627151818939\n",
      "Epoch 569 Loss: 0.007023480443858017\n",
      "Epoch 570 Loss: 0.007023333995591909\n",
      "Epoch 571 Loss: 0.007023187806426598\n",
      "Epoch 572 Loss: 0.007023041875769291\n",
      "Epoch 573 Loss: 0.007022896203028451\n",
      "Epoch 574 Loss: 0.007022750787613774\n",
      "Epoch 575 Loss: 0.0070226056289361995\n",
      "Epoch 576 Loss: 0.007022460726407897\n",
      "Epoch 577 Loss: 0.007022316079442275\n",
      "Epoch 578 Loss: 0.00702217168745398\n",
      "Epoch 579 Loss: 0.007022027549858877\n",
      "Epoch 580 Loss: 0.0070218836660740774\n",
      "Epoch 581 Loss: 0.007021740035517918\n",
      "Epoch 582 Loss: 0.007021596657609959\n",
      "Epoch 583 Loss: 0.007021453531770991\n",
      "Epoch 584 Loss: 0.007021310657423031\n",
      "Epoch 585 Loss: 0.007021168033989321\n",
      "Epoch 586 Loss: 0.007021025660894319\n",
      "Epoch 587 Loss: 0.007020883537563702\n",
      "Epoch 588 Loss: 0.007020741663424382\n",
      "Epoch 589 Loss: 0.007020600037904472\n",
      "Epoch 590 Loss: 0.007020458660433305\n",
      "Epoch 591 Loss: 0.007020317530441441\n",
      "Epoch 592 Loss: 0.007020176647360634\n",
      "Epoch 593 Loss: 0.007020036010623868\n",
      "Epoch 594 Loss: 0.007019895619665316\n",
      "Epoch 595 Loss: 0.0070197554739203786\n",
      "Epoch 596 Loss: 0.007019615572825663\n",
      "Epoch 597 Loss: 0.007019475915818963\n",
      "Epoch 598 Loss: 0.007019336502339297\n",
      "Epoch 599 Loss: 0.007019197331826871\n",
      "Epoch 600 Loss: 0.007019058403723104\n",
      "Epoch 601 Loss: 0.007018919717470601\n",
      "Epoch 602 Loss: 0.007018781272513176\n",
      "Epoch 603 Loss: 0.007018643068295834\n",
      "Epoch 604 Loss: 0.007018505104264769\n",
      "Epoch 605 Loss: 0.00701836737986738\n",
      "Epoch 606 Loss: 0.007018229894552244\n",
      "Epoch 607 Loss: 0.007018092647769135\n",
      "Epoch 608 Loss: 0.007017955638969014\n",
      "Epoch 609 Loss: 0.007017818867604019\n",
      "Epoch 610 Loss: 0.007017682333127492\n",
      "Epoch 611 Loss: 0.0070175460349939335\n",
      "Epoch 612 Loss: 0.007017409972659044\n",
      "Epoch 613 Loss: 0.007017274145579696\n",
      "Epoch 614 Loss: 0.007017138553213941\n",
      "Epoch 615 Loss: 0.007017003195021\n",
      "Epoch 616 Loss: 0.0070168680704612765\n",
      "Epoch 617 Loss: 0.007016733178996342\n",
      "Epoch 618 Loss: 0.0070165985200889465\n",
      "Epoch 619 Loss: 0.007016464093202999\n",
      "Epoch 620 Loss: 0.0070163298978035814\n",
      "Epoch 621 Loss: 0.007016195933356939\n",
      "Epoch 622 Loss: 0.00701606219933048\n",
      "Epoch 623 Loss: 0.0070159286951927856\n",
      "Epoch 624 Loss: 0.00701579542041358\n",
      "Epoch 625 Loss: 0.007015662374463758\n",
      "Epoch 626 Loss: 0.007015529556815373\n",
      "Epoch 627 Loss: 0.007015396966941623\n",
      "Epoch 628 Loss: 0.007015264604316871\n",
      "Epoch 629 Loss: 0.007015132468416627\n",
      "Epoch 630 Loss: 0.007015000558717547\n",
      "Epoch 631 Loss: 0.007014868874697443\n",
      "Epoch 632 Loss: 0.0070147374158352664\n",
      "Epoch 633 Loss: 0.007014606181611122\n",
      "Epoch 634 Loss: 0.007014475171506249\n",
      "Epoch 635 Loss: 0.007014344385003028\n",
      "Epoch 636 Loss: 0.007014213821584984\n",
      "Epoch 637 Loss: 0.007014083480736781\n",
      "Epoch 638 Loss: 0.007013953361944216\n",
      "Epoch 639 Loss: 0.0070138234646942126\n",
      "Epoch 640 Loss: 0.007013693788474836\n",
      "Epoch 641 Loss: 0.007013564332775287\n",
      "Epoch 642 Loss: 0.007013435097085877\n",
      "Epoch 643 Loss: 0.007013306080898059\n",
      "Epoch 644 Loss: 0.007013177283704409\n",
      "Epoch 645 Loss: 0.00701304870499862\n",
      "Epoch 646 Loss: 0.007012920344275506\n",
      "Epoch 647 Loss: 0.007012792201031013\n",
      "Epoch 648 Loss: 0.007012664274762189\n",
      "Epoch 649 Loss: 0.007012536564967211\n",
      "Epoch 650 Loss: 0.007012409071145358\n",
      "Epoch 651 Loss: 0.007012281792797028\n",
      "Epoch 652 Loss: 0.007012154729423728\n",
      "Epoch 653 Loss: 0.0070120278805280815\n",
      "Epoch 654 Loss: 0.007011901245613802\n",
      "Epoch 655 Loss: 0.007011774824185717\n",
      "Epoch 656 Loss: 0.007011648615749759\n",
      "Epoch 657 Loss: 0.00701152261981296\n",
      "Epoch 658 Loss: 0.0070113968358834445\n",
      "Epoch 659 Loss: 0.007011271263470448\n",
      "Epoch 660 Loss: 0.007011145902084287\n",
      "Epoch 661 Loss: 0.007011020751236379\n",
      "Epoch 662 Loss: 0.007010895810439236\n",
      "Epoch 663 Loss: 0.007010771079206452\n",
      "Epoch 664 Loss: 0.007010646557052713\n",
      "Epoch 665 Loss: 0.007010522243493793\n",
      "Epoch 666 Loss: 0.007010398138046548\n",
      "Epoch 667 Loss: 0.007010274240228916\n",
      "Epoch 668 Loss: 0.007010150549559914\n",
      "Epoch 669 Loss: 0.007010027065559641\n",
      "Epoch 670 Loss: 0.00700990378774927\n",
      "Epoch 671 Loss: 0.007009780715651046\n",
      "Epoch 672 Loss: 0.0070096578487883\n",
      "Epoch 673 Loss: 0.0070095351866854205\n",
      "Epoch 674 Loss: 0.007009412728867866\n",
      "Epoch 675 Loss: 0.0070092904748621674\n",
      "Epoch 676 Loss: 0.0070091684241959185\n",
      "Epoch 677 Loss: 0.0070090465763977755\n",
      "Epoch 678 Loss: 0.007008924930997459\n",
      "Epoch 679 Loss: 0.007008803487525747\n",
      "Epoch 680 Loss: 0.0070086822455144755\n",
      "Epoch 681 Loss: 0.007008561204496532\n",
      "Epoch 682 Loss: 0.007008440364005872\n",
      "Epoch 683 Loss: 0.0070083197235774825\n",
      "Epoch 684 Loss: 0.007008199282747411\n",
      "Epoch 685 Loss: 0.007008079041052759\n",
      "Epoch 686 Loss: 0.007007958998031668\n",
      "Epoch 687 Loss: 0.007007839153223315\n",
      "Epoch 688 Loss: 0.007007719506167941\n",
      "Epoch 689 Loss: 0.007007600056406803\n",
      "Epoch 690 Loss: 0.007007480803482212\n",
      "Epoch 691 Loss: 0.007007361746937514\n",
      "Epoch 692 Loss: 0.007007242886317079\n",
      "Epoch 693 Loss: 0.0070071242211663266\n",
      "Epoch 694 Loss: 0.007007005751031695\n",
      "Epoch 695 Loss: 0.0070068874754606525\n",
      "Epoch 696 Loss: 0.007006769394001698\n",
      "Epoch 697 Loss: 0.007006651506204354\n",
      "Epoch 698 Loss: 0.007006533811619164\n",
      "Epoch 699 Loss: 0.007006416309797692\n",
      "Epoch 700 Loss: 0.00700629900029253\n",
      "Epoch 701 Loss: 0.0070061818826572765\n",
      "Epoch 702 Loss: 0.00700606495644655\n",
      "Epoch 703 Loss: 0.00700594822121598\n",
      "Epoch 704 Loss: 0.007005831676522211\n",
      "Epoch 705 Loss: 0.007005715321922892\n",
      "Epoch 706 Loss: 0.007005599156976692\n",
      "Epoch 707 Loss: 0.007005483181243267\n",
      "Epoch 708 Loss: 0.007005367394283291\n",
      "Epoch 709 Loss: 0.007005251795658432\n",
      "Epoch 710 Loss: 0.007005136384931364\n",
      "Epoch 711 Loss: 0.0070050211616657495\n",
      "Epoch 712 Loss: 0.007004906125426259\n",
      "Epoch 713 Loss: 0.007004791275778552\n",
      "Epoch 714 Loss: 0.007004676612289273\n",
      "Epoch 715 Loss: 0.0070045621345260715\n",
      "Epoch 716 Loss: 0.007004447842057567\n",
      "Epoch 717 Loss: 0.007004333734453381\n",
      "Epoch 718 Loss: 0.00700421981128411\n",
      "Epoch 719 Loss: 0.007004106072121337\n",
      "Epoch 720 Loss: 0.007003992516537629\n",
      "Epoch 721 Loss: 0.007003879144106517\n",
      "Epoch 722 Loss: 0.007003765954402529\n",
      "Epoch 723 Loss: 0.007003652947001149\n",
      "Epoch 724 Loss: 0.007003540121478848\n",
      "Epoch 725 Loss: 0.007003427477413057\n",
      "Epoch 726 Loss: 0.007003315014382185\n",
      "Epoch 727 Loss: 0.007003202731965598\n",
      "Epoch 728 Loss: 0.007003090629743636\n",
      "Epoch 729 Loss: 0.007002978707297596\n",
      "Epoch 730 Loss: 0.007002866964209744\n",
      "Epoch 731 Loss: 0.007002755400063291\n",
      "Epoch 732 Loss: 0.00700264401444242\n",
      "Epoch 733 Loss: 0.0070025328069322655\n",
      "Epoch 734 Loss: 0.007002421777118901\n",
      "Epoch 735 Loss: 0.007002310924589377\n",
      "Epoch 736 Loss: 0.0070022002489316705\n",
      "Epoch 737 Loss: 0.007002089749734721\n",
      "Epoch 738 Loss: 0.0070019794265884045\n",
      "Epoch 739 Loss: 0.007001869279083542\n",
      "Epoch 740 Loss: 0.007001759306811906\n",
      "Epoch 741 Loss: 0.0070016495093661955\n",
      "Epoch 742 Loss: 0.007001539886340053\n",
      "Epoch 743 Loss: 0.007001430437328059\n",
      "Epoch 744 Loss: 0.007001321161925723\n",
      "Epoch 745 Loss: 0.0070012120597294945\n",
      "Epoch 746 Loss: 0.007001103130336748\n",
      "Epoch 747 Loss: 0.007000994373345786\n",
      "Epoch 748 Loss: 0.007000885788355831\n",
      "Epoch 749 Loss: 0.007000777374967048\n",
      "Epoch 750 Loss: 0.007000669132780503\n",
      "Epoch 751 Loss: 0.007000561061398203\n",
      "Epoch 752 Loss: 0.007000453160423059\n",
      "Epoch 753 Loss: 0.0070003454294589006\n",
      "Epoch 754 Loss: 0.007000237868110482\n",
      "Epoch 755 Loss: 0.007000130475983455\n",
      "Epoch 756 Loss: 0.007000023252684394\n",
      "Epoch 757 Loss: 0.0069999161978207824\n",
      "Epoch 758 Loss: 0.006999809311001008\n",
      "Epoch 759 Loss: 0.006999702591834354\n",
      "Epoch 760 Loss: 0.006999596039931026\n",
      "Epoch 761 Loss: 0.006999489654902122\n",
      "Epoch 762 Loss: 0.006999383436359632\n",
      "Epoch 763 Loss: 0.0069992773839164515\n",
      "Epoch 764 Loss: 0.0069991714971863765\n",
      "Epoch 765 Loss: 0.006999065775784086\n",
      "Epoch 766 Loss: 0.006998960219325157\n",
      "Epoch 767 Loss: 0.006998854827426057\n",
      "Epoch 768 Loss: 0.006998749599704138\n",
      "Epoch 769 Loss: 0.006998644535777641\n",
      "Epoch 770 Loss: 0.00699853963526569\n",
      "Epoch 771 Loss: 0.006998434897788295\n",
      "Epoch 772 Loss: 0.006998330322966342\n",
      "Epoch 773 Loss: 0.006998225910421593\n",
      "Epoch 774 Loss: 0.0069981216597766955\n",
      "Epoch 775 Loss: 0.006998017570655166\n",
      "Epoch 776 Loss: 0.006997913642681393\n",
      "Epoch 777 Loss: 0.006997809875480643\n",
      "Epoch 778 Loss: 0.00699770626867904\n",
      "Epoch 779 Loss: 0.00699760282190359\n",
      "Epoch 780 Loss: 0.00699749953478215\n",
      "Epoch 781 Loss: 0.006997396406943449\n",
      "Epoch 782 Loss: 0.006997293438017082\n",
      "Epoch 783 Loss: 0.006997190627633484\n",
      "Epoch 784 Loss: 0.006997087975423975\n",
      "Epoch 785 Loss: 0.006996985481020709\n",
      "Epoch 786 Loss: 0.006996883144056709\n",
      "Epoch 787 Loss: 0.006996780964165837\n",
      "Epoch 788 Loss: 0.00699667894098282\n",
      "Epoch 789 Loss: 0.0069965770741432206\n",
      "Epoch 790 Loss: 0.006996475363283456\n",
      "Epoch 791 Loss: 0.006996373808040779\n",
      "Epoch 792 Loss: 0.006996272408053304\n",
      "Epoch 793 Loss: 0.006996171162959965\n",
      "Epoch 794 Loss: 0.0069960700724005535\n",
      "Epoch 795 Loss: 0.0069959691360156825\n",
      "Epoch 796 Loss: 0.0069958683534468045\n",
      "Epoch 797 Loss: 0.0069957677243362194\n",
      "Epoch 798 Loss: 0.0069956672483270435\n",
      "Epoch 799 Loss: 0.006995566925063226\n",
      "Epoch 800 Loss: 0.00699546675418955\n",
      "Epoch 801 Loss: 0.006995366735351621\n",
      "Epoch 802 Loss: 0.0069952668681958685\n",
      "Epoch 803 Loss: 0.006995167152369547\n",
      "Epoch 804 Loss: 0.006995067587520728\n",
      "Epoch 805 Loss: 0.006994968173298304\n",
      "Epoch 806 Loss: 0.006994868909351987\n",
      "Epoch 807 Loss: 0.006994769795332303\n",
      "Epoch 808 Loss: 0.006994670830890587\n",
      "Epoch 809 Loss: 0.006994572015678994\n",
      "Epoch 810 Loss: 0.006994473349350482\n",
      "Epoch 811 Loss: 0.006994374831558818\n",
      "Epoch 812 Loss: 0.00699427646195858\n",
      "Epoch 813 Loss: 0.006994178240205143\n",
      "Epoch 814 Loss: 0.006994080165954693\n",
      "Epoch 815 Loss: 0.00699398223886421\n",
      "Epoch 816 Loss: 0.006993884458591469\n",
      "Epoch 817 Loss: 0.006993786824795061\n",
      "Epoch 818 Loss: 0.006993689337134348\n",
      "Epoch 819 Loss: 0.006993591995269504\n",
      "Epoch 820 Loss: 0.006993494798861485\n",
      "Epoch 821 Loss: 0.00699339774757204\n",
      "Epoch 822 Loss: 0.006993300841063708\n",
      "Epoch 823 Loss: 0.006993204078999808\n",
      "Epoch 824 Loss: 0.0069931074610444545\n",
      "Epoch 825 Loss: 0.00699301098686253\n",
      "Epoch 826 Loss: 0.006992914656119716\n",
      "Epoch 827 Loss: 0.006992818468482453\n",
      "Epoch 828 Loss: 0.006992722423617979\n",
      "Epoch 829 Loss: 0.006992626521194287\n",
      "Epoch 830 Loss: 0.00699253076088016\n",
      "Epoch 831 Loss: 0.006992435142345152\n",
      "Epoch 832 Loss: 0.006992339665259577\n",
      "Epoch 833 Loss: 0.006992244329294527\n",
      "Epoch 834 Loss: 0.0069921491341218564\n",
      "Epoch 835 Loss: 0.006992054079414183\n",
      "Epoch 836 Loss: 0.006991959164844893\n",
      "Epoch 837 Loss: 0.006991864390088133\n",
      "Epoch 838 Loss: 0.006991769754818804\n",
      "Epoch 839 Loss: 0.006991675258712573\n",
      "Epoch 840 Loss: 0.006991580901445855\n",
      "Epoch 841 Loss: 0.006991486682695825\n",
      "Epoch 842 Loss: 0.006991392602140415\n",
      "Epoch 843 Loss: 0.006991298659458288\n",
      "Epoch 844 Loss: 0.00699120485432888\n",
      "Epoch 845 Loss: 0.006991111186432366\n",
      "Epoch 846 Loss: 0.006991017655449667\n",
      "Epoch 847 Loss: 0.006990924261062436\n",
      "Epoch 848 Loss: 0.00699083100295309\n",
      "Epoch 849 Loss: 0.0069907378808047715\n",
      "Epoch 850 Loss: 0.006990644894301367\n",
      "Epoch 851 Loss: 0.006990552043127493\n",
      "Epoch 852 Loss: 0.006990459326968517\n",
      "Epoch 853 Loss: 0.0069903667455105244\n",
      "Epoch 854 Loss: 0.006990274298440336\n",
      "Epoch 855 Loss: 0.0069901819854455115\n",
      "Epoch 856 Loss: 0.0069900898062143346\n",
      "Epoch 857 Loss: 0.006989997760435813\n",
      "Epoch 858 Loss: 0.006989905847799675\n",
      "Epoch 859 Loss: 0.00698981406799638\n",
      "Epoch 860 Loss: 0.006989722420717111\n",
      "Epoch 861 Loss: 0.006989630905653766\n",
      "Epoch 862 Loss: 0.006989539522498962\n",
      "Epoch 863 Loss: 0.0069894482709460335\n",
      "Epoch 864 Loss: 0.006989357150689031\n",
      "Epoch 865 Loss: 0.006989266161422719\n",
      "Epoch 866 Loss: 0.006989175302842562\n",
      "Epoch 867 Loss: 0.006989084574644756\n",
      "Epoch 868 Loss: 0.006988993976526187\n",
      "Epoch 869 Loss: 0.006988903508184458\n",
      "Epoch 870 Loss: 0.006988813169317866\n",
      "Epoch 871 Loss: 0.006988722959625418\n",
      "Epoch 872 Loss: 0.00698863287880683\n",
      "Epoch 873 Loss: 0.0069885429265625024\n",
      "Epoch 874 Loss: 0.00698845310259355\n",
      "Epoch 875 Loss: 0.006988363406601766\n",
      "Epoch 876 Loss: 0.0069882738382896535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 877 Loss: 0.006988184397360406\n",
      "Epoch 878 Loss: 0.006988095083517903\n",
      "Epoch 879 Loss: 0.006988005896466715\n",
      "Epoch 880 Loss: 0.006987916835912108\n",
      "Epoch 881 Loss: 0.006987827901560024\n",
      "Epoch 882 Loss: 0.0069877390931171015\n",
      "Epoch 883 Loss: 0.006987650410290655\n",
      "Epoch 884 Loss: 0.006987561852788676\n",
      "Epoch 885 Loss: 0.006987473420319854\n",
      "Epoch 886 Loss: 0.006987385112593536\n",
      "Epoch 887 Loss: 0.006987296929319756\n",
      "Epoch 888 Loss: 0.006987208870209229\n",
      "Epoch 889 Loss: 0.0069871209349733265\n",
      "Epoch 890 Loss: 0.006987033123324114\n",
      "Epoch 891 Loss: 0.006986945434974307\n",
      "Epoch 892 Loss: 0.006986857869637305\n",
      "Epoch 893 Loss: 0.006986770427027165\n",
      "Epoch 894 Loss: 0.0069866831068586144\n",
      "Epoch 895 Loss: 0.006986595908847044\n",
      "Epoch 896 Loss: 0.006986508832708502\n",
      "Epoch 897 Loss: 0.006986421878159709\n",
      "Epoch 898 Loss: 0.006986335044918028\n",
      "Epoch 899 Loss: 0.006986248332701495\n",
      "Epoch 900 Loss: 0.0069861617412287965\n",
      "Epoch 901 Loss: 0.006986075270219268\n",
      "Epoch 902 Loss: 0.006985988919392911\n",
      "Epoch 903 Loss: 0.0069859026884703645\n",
      "Epoch 904 Loss: 0.006985816577172917\n",
      "Epoch 905 Loss: 0.006985730585222523\n",
      "Epoch 906 Loss: 0.006985644712341764\n",
      "Epoch 907 Loss: 0.006985558958253872\n",
      "Epoch 908 Loss: 0.00698547332268273\n",
      "Epoch 909 Loss: 0.0069853878053528495\n",
      "Epoch 910 Loss: 0.006985302405989397\n",
      "Epoch 911 Loss: 0.006985217124318168\n",
      "Epoch 912 Loss: 0.0069851319600656\n",
      "Epoch 913 Loss: 0.00698504691295876\n",
      "Epoch 914 Loss: 0.006984961982725355\n",
      "Epoch 915 Loss: 0.006984877169093719\n",
      "Epoch 916 Loss: 0.00698479247179283\n",
      "Epoch 917 Loss: 0.006984707890552278\n",
      "Epoch 918 Loss: 0.006984623425102288\n",
      "Epoch 919 Loss: 0.006984539075173715\n",
      "Epoch 920 Loss: 0.006984454840498036\n",
      "Epoch 921 Loss: 0.006984370720807352\n",
      "Epoch 922 Loss: 0.006984286715834387\n",
      "Epoch 923 Loss: 0.006984202825312484\n",
      "Epoch 924 Loss: 0.006984119048975595\n",
      "Epoch 925 Loss: 0.006984035386558309\n",
      "Epoch 926 Loss: 0.006983951837795822\n",
      "Epoch 927 Loss: 0.006983868402423934\n",
      "Epoch 928 Loss: 0.006983785080179069\n",
      "Epoch 929 Loss: 0.0069837018707982635\n",
      "Epoch 930 Loss: 0.006983618774019158\n",
      "Epoch 931 Loss: 0.006983535789580005\n",
      "Epoch 932 Loss: 0.0069834529172196555\n",
      "Epoch 933 Loss: 0.006983370156677581\n",
      "Epoch 934 Loss: 0.006983287507693844\n",
      "Epoch 935 Loss: 0.006983204970009112\n",
      "Epoch 936 Loss: 0.0069831225433646515\n",
      "Epoch 937 Loss: 0.006983040227502339\n",
      "Epoch 938 Loss: 0.0069829580221646426\n",
      "Epoch 939 Loss: 0.006982875927094615\n",
      "Epoch 940 Loss: 0.0069827939420359225\n",
      "Epoch 941 Loss: 0.006982712066732813\n",
      "Epoch 942 Loss: 0.006982630300930135\n",
      "Epoch 943 Loss: 0.006982548644373317\n",
      "Epoch 944 Loss: 0.006982467096808383\n",
      "Epoch 945 Loss: 0.006982385657981945\n",
      "Epoch 946 Loss: 0.0069823043276412014\n",
      "Epoch 947 Loss: 0.00698222310553393\n",
      "Epoch 948 Loss: 0.0069821419914084994\n",
      "Epoch 949 Loss: 0.006982060985013854\n",
      "Epoch 950 Loss: 0.006981980086099522\n",
      "Epoch 951 Loss: 0.006981899294415608\n",
      "Epoch 952 Loss: 0.0069818186097127975\n",
      "Epoch 953 Loss: 0.0069817380317423496\n",
      "Epoch 954 Loss: 0.0069816575602561\n",
      "Epoch 955 Loss: 0.00698157719500645\n",
      "Epoch 956 Loss: 0.0069814969357463825\n",
      "Epoch 957 Loss: 0.006981416782229448\n",
      "Epoch 958 Loss: 0.0069813367342097645\n",
      "Epoch 959 Loss: 0.006981256791442022\n",
      "Epoch 960 Loss: 0.0069811769536814615\n",
      "Epoch 961 Loss: 0.006981097220683914\n",
      "Epoch 962 Loss: 0.006981017592205748\n",
      "Epoch 963 Loss: 0.006980938068003913\n",
      "Epoch 964 Loss: 0.006980858647835912\n",
      "Epoch 965 Loss: 0.0069807793314598035\n",
      "Epoch 966 Loss: 0.006980700118634205\n",
      "Epoch 967 Loss: 0.006980621009118302\n",
      "Epoch 968 Loss: 0.006980542002671812\n",
      "Epoch 969 Loss: 0.006980463099055032\n",
      "Epoch 970 Loss: 0.006980384298028789\n",
      "Epoch 971 Loss: 0.006980305599354475\n",
      "Epoch 972 Loss: 0.006980227002794026\n",
      "Epoch 973 Loss: 0.00698014850810993\n",
      "Epoch 974 Loss: 0.006980070115065215\n",
      "Epoch 975 Loss: 0.006979991823423461\n",
      "Epoch 976 Loss: 0.006979913632948785\n",
      "Epoch 977 Loss: 0.006979835543405854\n",
      "Epoch 978 Loss: 0.006979757554559869\n",
      "Epoch 979 Loss: 0.00697967966617658\n",
      "Epoch 980 Loss: 0.006979601878022269\n",
      "Epoch 981 Loss: 0.006979524189863752\n",
      "Epoch 982 Loss: 0.006979446601468389\n",
      "Epoch 983 Loss: 0.006979369112604072\n",
      "Epoch 984 Loss: 0.0069792917230392226\n",
      "Epoch 985 Loss: 0.006979214432542794\n",
      "Epoch 986 Loss: 0.006979137240884276\n",
      "Epoch 987 Loss: 0.006979060147833689\n",
      "Epoch 988 Loss: 0.006978983153161565\n",
      "Epoch 989 Loss: 0.006978906256638984\n",
      "Epoch 990 Loss: 0.00697882945803753\n",
      "Epoch 991 Loss: 0.006978752757129334\n",
      "Epoch 992 Loss: 0.00697867615368703\n",
      "Epoch 993 Loss: 0.006978599647483781\n",
      "Epoch 994 Loss: 0.006978523238293271\n",
      "Epoch 995 Loss: 0.006978446925889698\n",
      "Epoch 996 Loss: 0.006978370710047789\n",
      "Epoch 997 Loss: 0.006978294590542765\n",
      "Epoch 998 Loss: 0.006978218567150389\n",
      "Epoch 999 Loss: 0.006978142639646912\n",
      "Epoch 1000 Loss: 0.006978066807809116\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array(([2, 9], [1, 5]), dtype=float)\n",
    "y1 = np.array(([92], [86]), dtype=float)\n",
    "\n",
    "# scale units\n",
    "X1, y1 = scale(X1, y1, Ymax=100)\n",
    "\n",
    "test1 = nNetwork(2,1,[4,6,4],sigmoid)\n",
    "test1.train(X1, y1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bee = None\n",
    "if bee:\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
